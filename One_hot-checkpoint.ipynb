{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.backend import one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess=tf.Session()\n",
    "iris=datasets.load_iris()\n",
    "t=one_hot(iris['target'],3) #(one_hot처리할 데이터 지정, 분류할 개수)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # one_ hot encoding\n",
    " \n",
    " - 장미 / 튤립 / 백합은 서로 관계 없는 꽃의 종류일지라도 숫자로 표현 되어있기 때문에 학습을 할 때 영향을 미칠 수 있다.\n",
    " \n",
    " - 꽃의 종류가 3 가지라면, 별도의 3개의 column을 만들고 3개의 column 중 해당 타입의 column 에만 1, 다른 column은 0을 대입해주는 pre-processing 을 거쳐야 하고, 이는 곧 one-hot encoding 의 기법이다.\n",
    " \n",
    "- 즉, 다음과 같이 바뀌게 된다.[1, 0, 0] [0, 1, 0] [0, 0, 1] [0, 1, 0] [0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.670828\n",
      "1 1.1724482\n",
      "2 1.0912417\n",
      "3 1.0228095\n",
      "4 0.96776736\n",
      "5 0.9211986\n",
      "6 0.8888795\n",
      "7 0.86479515\n",
      "8 0.8567277\n",
      "9 0.8494075\n",
      "10 0.85962003\n",
      "11 0.8416771\n",
      "12 0.85521585\n",
      "13 0.81493187\n",
      "14 0.83194155\n",
      "15 0.78322613\n",
      "16 0.8070685\n",
      "17 0.7555266\n",
      "18 0.7853813\n",
      "19 0.7322254\n",
      "20 0.76664376\n",
      "21 0.71234906\n",
      "22 0.75020623\n",
      "23 0.6950792\n",
      "24 0.73554283\n",
      "25 0.67982715\n",
      "26 0.72226816\n",
      "27 0.6661681\n",
      "28 0.71009845\n",
      "29 0.6537875\n",
      "30 0.6988201\n",
      "31 0.64244854\n",
      "32 0.68827134\n",
      "33 0.631969\n",
      "34 0.67832416\n",
      "35 0.6222059\n",
      "36 0.6688795\n",
      "37 0.61304736\n",
      "38 0.65985835\n",
      "39 0.6044015\n",
      "40 0.65119547\n",
      "41 0.59619504\n",
      "42 0.64283884\n",
      "43 0.58836764\n",
      "44 0.634746\n",
      "45 0.58086866\n",
      "46 0.6268807\n",
      "47 0.57365626\n",
      "48 0.6192128\n",
      "49 0.56669563\n",
      "50 0.6117188\n",
      "51 0.5599558\n",
      "52 0.604375\n",
      "53 0.5534114\n",
      "54 0.59716505\n",
      "55 0.54704094\n",
      "56 0.5900735\n",
      "57 0.54082507\n",
      "58 0.58308685\n",
      "59 0.5347472\n",
      "60 0.576194\n",
      "61 0.52879286\n",
      "62 0.5693848\n",
      "63 0.5229494\n",
      "64 0.5626511\n",
      "65 0.5172062\n",
      "66 0.5559855\n",
      "67 0.511553\n",
      "68 0.5493823\n",
      "69 0.5059823\n",
      "70 0.54283637\n",
      "71 0.5004852\n",
      "72 0.5363417\n",
      "73 0.49505535\n",
      "74 0.5298957\n",
      "75 0.4896871\n",
      "76 0.5234939\n",
      "77 0.48437408\n",
      "78 0.5171337\n",
      "79 0.47911286\n",
      "80 0.5108128\n",
      "81 0.4738984\n",
      "82 0.50452965\n",
      "83 0.4687277\n",
      "84 0.49828222\n",
      "85 0.46359655\n",
      "86 0.49206838\n",
      "87 0.45850214\n",
      "88 0.48588815\n",
      "89 0.4534427\n",
      "90 0.47974095\n",
      "91 0.44841543\n",
      "92 0.47362605\n",
      "93 0.44341853\n",
      "94 0.4675429\n",
      "95 0.43844965\n",
      "96 0.46149135\n",
      "97 0.43350816\n",
      "98 0.4554725\n",
      "99 0.4285929\n",
      "100 0.44948623\n",
      "101 0.42370266\n",
      "102 0.44353354\n",
      "103 0.41883677\n",
      "104 0.43761516\n",
      "105 0.4139948\n",
      "106 0.43173203\n",
      "107 0.40917674\n",
      "108 0.42588598\n",
      "109 0.40438226\n",
      "110 0.42007804\n",
      "111 0.39961183\n",
      "112 0.4143102\n",
      "113 0.39486608\n",
      "114 0.40858468\n",
      "115 0.39014548\n",
      "116 0.40290344\n",
      "117 0.38545075\n",
      "118 0.3972687\n",
      "119 0.3807834\n",
      "120 0.39168376\n",
      "121 0.37614527\n",
      "122 0.38615158\n",
      "123 0.37153757\n",
      "124 0.38067526\n",
      "125 0.36696276\n",
      "126 0.3752587\n",
      "127 0.36242336\n",
      "128 0.36990592\n",
      "129 0.35792196\n",
      "130 0.36462134\n",
      "131 0.35346213\n",
      "132 0.35940978\n",
      "133 0.34904724\n",
      "134 0.3542764\n",
      "135 0.34468177\n",
      "136 0.34922758\n",
      "137 0.34037057\n",
      "138 0.34426907\n",
      "139 0.33611834\n",
      "140 0.33940718\n",
      "141 0.33193082\n",
      "142 0.33464956\n",
      "143 0.3278146\n",
      "144 0.33000353\n",
      "145 0.3237765\n",
      "146 0.32547772\n",
      "147 0.31982413\n",
      "148 0.32108045\n",
      "149 0.31596544\n",
      "150 0.31682026\n",
      "151 0.31220877\n",
      "152 0.31270638\n",
      "153 0.30856317\n",
      "154 0.30874756\n",
      "155 0.30503744\n",
      "156 0.30495256\n",
      "157 0.30164057\n",
      "158 0.30132926\n",
      "159 0.29838115\n",
      "160 0.29788494\n",
      "161 0.2952671\n",
      "162 0.29462525\n",
      "163 0.29230514\n",
      "164 0.29155445\n",
      "165 0.28950083\n",
      "166 0.2886746\n",
      "167 0.28685752\n",
      "168 0.28598502\n",
      "169 0.28437632\n",
      "170 0.2834826\n",
      "171 0.2820559\n",
      "172 0.28116134\n",
      "173 0.279892\n",
      "174 0.27901226\n",
      "175 0.27787796\n",
      "176 0.27702415\n",
      "177 0.27600428\n",
      "178 0.27518362\n",
      "179 0.27425966\n",
      "180 0.27347544\n",
      "181 0.2726313\n",
      "182 0.271884\n",
      "183 0.27110532\n",
      "184 0.2703934\n",
      "185 0.26966792\n",
      "186 0.26898828\n",
      "187 0.2683056\n",
      "188 0.26765463\n",
      "189 0.26700613\n",
      "190 0.26637968\n",
      "191 0.2657583\n",
      "192 0.26515263\n",
      "193 0.26455286\n",
      "194 0.26396453\n",
      "early stopping\n"
     ]
    }
   ],
   "source": [
    "ydata = sess.run(t)\n",
    "xdata = iris['data']\n",
    "x=tf.placeholder(tf.float32, shape=[None, 4])\n",
    "y=tf.constant(ydata, tf.float32)\n",
    "w=tf.Variable(tf.random_uniform([4,3]))\n",
    "b=tf.Variable(tf.random_uniform([3]))\n",
    "z=tf.matmul(x,w)+b\n",
    "hx=tf.nn.softmax(z)\n",
    "\n",
    "cost_1=tf.nn.softmax_cross_entropy_with_logits_v2( labels=y, logits=z)\n",
    "cost=tf.reduce_mean(cost_1)\n",
    "optimizer=tf.train.GradientDescentOptimizer(0.1)\n",
    "train=optimizer.minimize(cost)\n",
    "sess=tf.Session()\n",
    "init=tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "hist_loss=[]\n",
    "patience=16\n",
    "min_delta=0.001\n",
    "for i in range(1000):\n",
    "    sess.run(train,{x:xdata})\n",
    "    c=sess.run(cost,{x:xdata})\n",
    "    hist_loss.append(c)\n",
    "    print(i,c)\n",
    "    if i>0:\n",
    "        if hist_loss[i-1]-hist_loss[i]>min_delta:\n",
    "            pcnt=0\n",
    "        else:\n",
    "            pcnt+=1\n",
    "        if pcnt>patience:\n",
    "            print('early stopping')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.종분류예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SL 5.1 SW 3.5 PL 1.4 PW 0.2 \n",
    "\n",
    "a = sess.run(hx,{x:[[5.1,3.5,1.4,0.2]]})\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'setosa'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type = ['setosa', 'versicolor', 'virginica']\n",
    "type[int(a.argmax())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9733333333333334"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = sess.run(hx, {x:xdata})\n",
    "(b.argmax(axis=1) == iris['target']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris['target']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
